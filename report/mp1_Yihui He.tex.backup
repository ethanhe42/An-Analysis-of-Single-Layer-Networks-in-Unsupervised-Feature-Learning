\documentclass[journal, a4paper, 11pt]{IEEEtran}

% some very useful LaTeX packages include:

%\usepackage{cite}      % Written by Donald Arseneau
                        % V1.6 and later of IEEEtran pre-defines the format
                        % of the cite.sty package \cite{} output to follow
                        % that of IEEE. Loading the cite package will
                        % result in citation numbers being automatically
                        % sorted and properly "ranged". i.e.,
                        % [1], [9], [2], [7], [5], [6]
                        % (without using cite.sty)
                        % will become:
                        % [1], [2], [5]--[7], [9] (using cite.sty)
                        % cite.sty's \cite will automatically add leading
                        % space, if needed. Use cite.sty's noadjust option
                        % (cite.sty V3.8 and later) if you want to turn this
                        % off. cite.sty is already installed on most LaTeX
                        % systems. The latest version can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/supported/cite/

\usepackage{graphicx}   % Written by David Carlisle and Sebastian Rahtz
                        % Required if you want graphics, photos, etc.
                        % graphicx.sty is already installed on most LaTeX
                        % systems. The latest version and documentation can
                        % be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/required/graphics/
                        % Another good source of documentation is "Using
                        % Imported Graphics in LaTeX2e" by Keith Reckdahl
                        % which can be found as esplatex.ps and epslatex.pdf
                        % at: http://www.ctan.org/tex-archive/info/

%\usepackage{psfrag}    % Written by Craig Barratt, Michael C. Grant,
                        % and David Carlisle
                        % This package allows you to substitute LaTeX
                        % commands for text in imported EPS graphic files.
                        % In this way, LaTeX symbols can be placed into
                        % graphics that have been generated by other
                        % applications. You must use latex->dvips->ps2pdf
                        % workflow (not direct pdf output from pdflatex) if
                        % you wish to use this capability because it works
                        % via some PostScript tricks. Alternatively, the
                        % graphics could be processed as separate files via
                        % psfrag and dvips, then converted to PDF for
                        % inclusion in the main file which uses pdflatex.
                        % Docs are in "The PSfrag System" by Michael C. Grant
                        % and David Carlisle. There is also some information
                        % about using psfrag in "Using Imported Graphics in
                        % LaTeX2e" by Keith Reckdahl which documents the
                        % graphicx package (see above). The psfrag package
                        % and documentation can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/supported/psfrag/

%\usepackage{subfigure} % Written by Steven Douglas Cochran
                        % This package makes it easy to put subfigures
                        % in your figures. i.e., "figure 1a and 1b"
                        % Docs are in "Using Imported Graphics in LaTeX2e"
                        % by Keith Reckdahl which also documents the graphicx
                        % package (see above). subfigure.sty is already
                        % installed on most LaTeX systems. The latest version
                        % and documentation can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/supported/subfigure/

\usepackage{url}        % Written by Donald Arseneau
                        % Provides better support for handling and breaking
                        % URLs. url.sty is already installed on most LaTeX
                        % systems. The latest version can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/other/misc/
                        % Read the url.sty source comments for usage information.

%\usepackage{stfloats}  % Written by Sigitas Tolusis
                        % Gives LaTeX2e the ability to do double column
                        % floats at the bottom of the page as well as the top.
                        % (e.g., "\begin{figure*}[!b]" is not normally
                        % possible in LaTeX2e). This is an invasive package
                        % which rewrites many portions of the LaTeX2e output
                        % routines. It may not work with other packages that
                        % modify the LaTeX2e output routine and/or with other
                        % versions of LaTeX. The latest version and
                        % documentation can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/supported/sttools/
                        % Documentation is contained in the stfloats.sty
                        % comments as well as in the presfull.pdf file.
                        % Do not use the stfloats baselinefloat ability as
                        % IEEE does not allow \baselineskip to stretch.
                        % Authors submitting work to the IEEE should note
                        % that IEEE rarely uses double column equations and
                        % that authors should try to avoid such use.
                        % Do not be tempted to use the cuted.sty or
                        % midfloat.sty package (by the same author) as IEEE
                        % does not format its papers in such ways.

\usepackage{amsmath}    % From the American Mathematical Society
                        % A popular package that provides many helpful commands
                        % for dealing with mathematics. Note that the AMSmath
                        % package sets \interdisplaylinepenalty to 10000 thus
                        % preventing page breaks from occurring within multiline
                        % equations. Use:
%\interdisplaylinepenalty=2500
                        % after loading amsmath to restore such page breaks
                        % as IEEEtran.cls normally does. amsmath.sty is already
                        % installed on most LaTeX systems. The latest version
                        % and documentation can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/



% Other popular packages for formatting tables and equations include:

%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty which improves the
% LaTeX2e array and tabular environments to provide better appearances and
% additional user controls. array.sty is already installed on most systems.
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/

% V1.6 of IEEEtran contains the IEEEeqnarray family of commands that can
% be used to generate multiline equations as well as matrices, tables, etc.

% Also of notable interest:
% Scott Pakin's eqparbox package for creating (automatically sized) equal
% width boxes. Available:
% http://www.ctan.org/tex-archive/macros/latex/contrib/supported/eqparbox/

% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.


\usepackage{listings}
\usepackage{xcolor}


\lstset{
	language=Python,
	basicstyle=\ttfamily,
	otherkeywords={self},             
	keywords=[2]{True,False},
	keywords=[3]{ttk},
	emph={MyClass,__init__},         
	showstringspaces=false, 
	aboveskip=0em,
	belowskip=0em,
	xleftmargin=1em,
	frame=l
}
\usepackage{epstopdf}

% Your document starts here!
\begin{document}

% Define document title and author
	\title{Machine Problem 1 Report}
	\author{Yihui He*
	\thanks{*Exchange student, 2nd year CS undergrad from  Xi'an Jiaotong University, yihui.xjtu@gmail.com}}
	\markboth{CS291K}{}
	\maketitle

% Write abstract here
\begin{abstract}
	The short abstract (50-80 words) is intended to give the reader an overview of the work.
\end{abstract}

% Each section begins with a \section{title} command
\section{Implementation}
% \PARstart{}{} creates a tall first letter for this first paragraph
	\PARstart{}{Key} points of implementing each part of neural network are illustrated as follow. \\
	All matrix multiplications are done via np.dot(). Element-wise matrix operation is done via "*".
\subsection{Forward pass and Loss}
	In Forward pass \textit{np.maximum} between z and 0, is used to represent \textit{ReLU} activation function.
	
	In Softmax, first raise \textit{scores} to the power of e element-wisely, then divide each element by row sum, finally we get $ a^{(3)} $.
	
	When computing Loss, pick up each true label element in each row is tricky: \textit{a3[range(len(a3)),y]}. Part of Forward pass is shown below. Other part of codes can be found in source files.
\begin{lstlisting}
a2=np.maximum(X.dot(W1)+b1,0)
scores=a2.dot(W2)+b2
a3=exp_scores\
  /(np.sum(exp_scores,1))[:,None]

\end{lstlisting}
\subsection{Backward pass and Gradient check}
	Firstly for each input, we need to compute $\delta_j^{(3)}$ for each output unit $j$:
	\[\delta_j^{(3)}=
	\begin{cases}
		\frac{1}{N}p(Y=j|X=x_i)-\frac{1}{N} &j=y_i\\ 
		 \frac{1}{N}p(Y=j|X=x_i) &j\neq y_i
	\end{cases}\]

\begin{lstlisting}
delta_3=a3
delta_3[range(len(a3)),y]=\
  a3[range(len(a3)),y]-1
delta_3/=len(a3)
grads['W2']=a2.T.dot(delta_3)+reg*W2
grads['b2']=np.sum(delta_3,0)
\end{lstlisting}

	Secondly, in the front hidden layer, for each input, we need to compute $\delta_j^{(3)}$ for each hidden node $j$:
	\[\delta_j^{(2)}= (w_j^{(2)})^T\delta^{(3)}\circ f'(z_j^{(2)})\]
	Note that, the second operator is Hadamard product.
\begin{lstlisting}
dF=np.ones(np.shape(a2))
dF[a2==0.0]=0
delta_2=delta_3.dot(W2.T)*dF
grads['W1']=X.T.dot(delta_2)+reg*W1
grads['b1']=np.sum(delta_2,0)
\end{lstlisting}

	To avoid divide by zero in gradient check, I made a small modification to the formula:
	\[\frac{\left|A-B\right|}{max(10^{-8},\left|A+B\right|)}\leq \delta\]
	max errors among inputs are as follow:\\
	$w_1$ 3.56e-09  $b_1$   2.74e-09\\
	$w_2$ 3.44e-09 $b_2$   4.45e-11

\subsection{Train and Predict}
	To perform a minibatching, \textit{np.random.choice} can be used, and set \textit{replace=True} to avoid same inputs being used. Then update each hyperparameter using SGD.
\begin{lstlisting}
rand_idx=np.random.choice(
  num_train,batch_size,replace=False)
X_batch=X[rand_idx]
y_batch=y[rand_idx]
for var in self.params:
  self.params[var]-=\
    learning_rate*grads[var]
\end{lstlisting}
As for predicting, run forward propagation, and use \textit{np.argmax( ,1)}to find predicted \textit{y} for each input.
\begin{lstlisting}
y_pred=np.argmax(np.maximum(0,\
  (X.dot(self.params["W1"])\
  +self.params['b1']))\
  .dot(self.params['W2'])+\
  self.params['b2'],1)
\end{lstlisting}


\section{Model Building}
	Basically there are two ways to tune a neural net: grid search and random search. For simplicity, I employ grid search to tune 3 hyperparameters: number of neurons in the hidden layer, regularization strength, and learning rate. My tuning procedure is twofold. First, run a coarse-grained search. Second, based on the result of first step, run a fine-grained search around top results.
	
	For coarse-grained search, Number of neurons range from 50 to 550, step 50. Regularization strength and learning rate are selected from geometrical sequences. Regularization strength range from $0.5\times10^{-3}$ to $0.5\times10^{2}$, with ratio $10$. Learning rate range from $1\times10^{-5}$ to $1\times10^{-1}$, with ratio $10$. During this, top results have about \textbf{50\%} validation accuracy. Some Top results are shown below:

	% You can reference tables and figure by using the \ref{label} command. Each table and figure needs to have a UNIQUE label.
	Figures and tables should be labeled and numbered, such as in Table~\ref{tab:simParameters} and Fig.~\ref{fig:tf_plot}.
	
	% This is how you define a table: the [!hbt] means that LaTeX is forced (by the !) to place the table exactly here (by h), or if that doesnt work because of a pagebreak or so, it tries to place the table to the bottom of the page (by b) or the top (by t).
	\begin{table}[!hbt]
		\centering
		\caption{top accuracy}
		\label{top-accuracy}
		\begin{tabular}{|l|l|l|l|}
			\hline

			\multicolumn{1}{|p{1cm}|}{\centering hidden \\ neurons}
			
			&\multicolumn{1}{|p{1cm}|}{\centering learning \\ rate}
			&\multicolumn{1}{|p{2cm}|}{\centering regularization \\ strength}
			&\multicolumn{1}{|p{1.5cm}|}{\centering validation \\ accuracy}\\
			\hline
			350 & 0.001    & 0.05   & 0.516 \\
			\hline
			400 & 0.001    & 0.005  & 0.509 \\
						\hline
			250 & 0.001    & 0.0005 & 0.505 \\
						\hline
			250 & 0.001    & 0.05   & 0.501 \\
						\hline
			150 & 0.001    & 0.005  & 0.5   \\
						\hline
			500 & 0.001    & 0.05   & 0.5  \\
			\hline
	\end{tabular}
\end{table}
	For fine-grained search, I picked up one of the above top results. And search around it's hyperparameters. It can reach a validation accuracy of \textbf{52\%}. After found a suitable set of hyperparameters, I start tuning numbers of iterations, and batch size. Finally, our original neural network reach a validation accuracy of \textbf{56\%}.

\section{Extra Credits}
I put results in the next section, in order to compare our original two-layers neural network, with other enhanced neural networks.
\subsection{momentum and other update methods}
	Implementation of momentum needs to change hyperparameters update code in \emph{train}.
\begin{lstlisting}
self.cache[param]=np.zeros(
grads[param].shape)
self.cache[param]=arg*self.cache[param
-learning_rate*grads[param]
self.params[param]+=self.cache[param]
\end{lstlisting}
	In order to tune momentum parameter and compare result with SGD, all other hyperparameters are fixed. Intuitively, momentum should speed up training procedure. I test momentum and SGD with 1000 iterations to see their converge rate.  It turns out that enjoys better converge rates.
	
	I also tried other update methods. 
	Nesterov momentum, which is a look-ahead version of momentum.
\begin{lstlisting}
v_prev = cache[param]
cache[param]=arg*cache[param]\
  -learning_rate*grads[param]
self.params[param]+=-arg*v_prev\
  +(1+arg)*cache[param]
\end{lstlisting}
RMSprop, which is a per-parameter adaptive learning rate method.
\begin{lstlisting}
cache[param]=arg*cache[param]\
+(1-arg)*np.power(grads[param],2)
self.params[param]-=learning_rate\
*grads[param]\
/np.sqrt(cache[param]+1e-8)
\end{lstlisting}
It turns out that, Momentum, Nesterov momentum and RMSprop all have a better converge rate than SGD. However, difference between these three update methods is ambiguous. Performances of these methods are compared in table~\ref{Differences between update methods}
\begin{table}[!hbt]
	\centering
	\caption{Differences between update methods}
	\label{Differences between update methods}
	\begin{tabular}{lllll}
		accuracy & Train & Validation & Test &  \\
		SGD      & .27   & .28        & .28  &  \\
		Momentum & .49   & .472       & .458 &  \\
		Nesterov & .471  & .452       & .461 &  \\
		RMSprop  & .477  & .458       & .475 & 
	\end{tabular}
\end{table}
				
\subsection{Dropout}
	Because we only have one layer of hidden nodes, dropout only needs to be performed once per batch. In code, it only needs minor change to Forward pass. 
\begin{lstlisting}
a2*=(np.random.randn(*a2.shape)<p)/p
\end{lstlisting}
	Dropout is said to be a easy way to prevent overfitting, and it is also an ensemble of multi models which should enhance performance. So I test it with more hidden neurons(500 hidden neurons). Dropout rate is set to be 30\% , 50\% and 70\% empirically. It turns out that the results from these three dropout rates do not differ a lot, with test accuracy ranging from 54\% to 56\%. And it also shows that, with out L2 regularization, dropout is able to constrain train accuracy not too higher than validation accuracy, which is 73\%.
	
	% This is how you include a eps figure in your document. LaTeX only accepts EPS or TIFF files.

	\begin{figure}[!hbt]
		% Center the figure.
		\begin{center}
			% Include the eps file, scale it such that it's width equals the column width. You can also put width=8cm for example...
			\includegraphics[width=\columnwidth]{dropout}
			% Create a subtitle for the figure.
			\caption{Simulation results on the AWGN channel. Average throughput $k/n$ vs $E_s/N_0$.}
			% Define the label of the figure. It's good to use 'fig:title', so you know that the label belongs to a figure.
			\label{fig:tf_plot}
		\end{center}
	\end{figure}
	
	
\subsection{Initialization method}
I tried 3 different ways of initialization:$\text{N}(0,1)\sqrt{1/n}$,
$\text{N}(0,1)\sqrt{2/(n_{in} + n _{out}})$,$\text{N}(0,1)\sqrt{2/n}$. Comparing with the our original initialization: $10^{-4}\text{N}(0,1)$, some outperform it. The neural network I'm testing on have 500 hidden neurons. I trained it 10000 iterations, 100 batch size, with dropout and momentum.

 In our case, we have 3072 input neurons, so $\text{N}(0,1)\sqrt{1/n}$ actually is $3.3\times10^{-4}$, whose weights are 3 times bigger than our original initialization. It is terrible at first 100 iterations. It reaches loss of 33.161757. 
 
 We only 10 output neurons. So $\text{N}(0,1)\sqrt{2/(n_{in} + n _{out}})$ and $\text{N}(0,1)\sqrt{2/n}$ do not differ so much between each other. Although, these two initialization methods' weights are 1.5 times bigger than our original initialization.Loss changing is almost the same as our original initialization. But maybe these methods are better, they are related to the input and output rather than a hand setting value.
 
 	\begin{figure}[!hbt]
 		% Center the figure.
 		\begin{center}
 			% Include the eps file, scale it such that it's width equals the column width. You can also put width=8cm for example...
 			\includegraphics[width=\columnwidth]{loss}
 			% Create a subtitle for the figure.
 			\caption{Simulation results on the AWGN channel. Average throughput $k/n$ vs $E_s/N_0$.}
 			% Define the label of the figure. It's good to use 'fig:title', so you know that the label belongs to a figure.
 			\label{fig:tf_plot}
 		\end{center}
 	\end{figure}
 
 
	
\subsection{Activation functions}
	I've tried 2 other activation functions: \emph{leaky ReLU} and \emph{tanh}. 
	
	
	
\subsection{Preprocessing}
	
\section{Results}
	As mentioned in II Model Building, I employed a grid search to find best hyperparameters. In order to get better result, I tuned number of iterations, finally fix it to 12000, with a batch size of 100. It increases the Validation accuracy 5\%. 
    0.57
    0.51
    0.55
    \begin{table}[!hbt]
    	\centering
    	\caption{Differences between update methods}
    	\label{Differences between update methods}
    	\begin{tabular}{lllll}
    		         & Naive & A & B & \\
    		hidden nodes    & 350   & 500        & .516 & \\
    		learning rate & $1\times10^{-3}$   & $1\times10^{-4}$       & .458 &  \\
    		regularization & L2,0.05  & Dropout,0.5       & .461 &  \\
    		Activation  & ReLU  & Leaky ReLU       & .461 &  \\
    		Update method  & SGD  & Momentum,0.9       & .461 &  \\
    		Iterations & $1\times10^{4}$  & $1\times10^{4}$       & .461 &  \\    		
    		Batch size & 100  & 100       & .461 &  \\    		
    		Time(min)  & 15  & 80       & .475 &     		\\
    		Train accuracy  & 60\%  & .458       & .475 & \\
			Validation  & 55\%  & .458       & .475 &     	\\	
    		Test  & 51.6\%  & .458       & .475 &     		
    		
    	\end{tabular}
    \end{table}
    

% You can cite a book or paper by using \cite{reference}.
% The references will be defined at the end of this .tex file in the bibliography
References should be cited as numbers, and should be ordered by their appearance (example: ``... as shown in \cite{HOP96}, ...'').
Only references that are actually cited can be listed in the references section.
The references' format should be evident from the examples in this text.

References should be of academic character and should be published and accessible.
Your advisor can answer your questions regarding literature research.
You must cite all used sources.
Examples of good references include text books and scientific journals or conference proceedings.
If possible, citing internet pages should be avoided. In particular, Wikipedia is \emph{not} an appropriate reference in academic reports.
Avoiding references in languages other than English is recommended.






\section{Challenges}
	I encountered many challenges through this machine problem
	
	When I'm doing gradient check. I spent great amount of time finding bugs in my code, but still not able to figure out what's wrong with my code. Then, instead of directly comparing gradient errors, I cut gradient check procedure into pieces. Check derivatives sequentially: $\frac{\partial H}{\partial a^{(3)}}$,$\delta^{(3)}$,$\frac{\partial H}{\partial w^{(2)}}$,$\frac{\partial H}{\partial b^{(2)}}$,$\delta^{(2)}...$
	
	Tuning hyperparameters is time consuming, I run my program on cloud server all day and save results to \emph{CSV} files.
	
\section{Possible Improvements}
	There are some other update methods(Adam, Adagrad, etc) I haven't tried. Maybe they are better than Momentum and RMSprop.

% Now we need a bibliography:
\begin{thebibliography}{5}

	%Each item starts with a \bibitem{reference} command and the details thereafter.
	\bibitem{HOP96} % Transaction paper
	J.~Hagenauer, E.~Offer, and L.~Papke. Iterative decoding of binary block
	and convolutional codes. {\em IEEE Trans. Inform. Theory},
	vol.~42, no.~2, pp.~429¨C-445, Mar. 1996.

	\bibitem{MJH06} % Conference paper
	T.~Mayer, H.~Jenkac, and J.~Hagenauer. Turbo base-station cooperation for intercell interference cancellation. {\em IEEE Int. Conf. Commun. (ICC)}, Istanbul, Turkey, pp.~356--361, June 2006.

	\bibitem{Proakis} % Book
	J.~G.~Proakis. {\em Digital Communications}. McGraw-Hill Book Co.,
	New York, USA, 3rd edition, 1995.

	\bibitem{talk} % Web document
	F.~R.~Kschischang. Giving a talk: Guidelines for the Preparation and Presentation of Technical Seminars.
	\url{http://www.comm.toronto.edu/frank/guide/guide.pdf}.

	\bibitem{5}
	IEEE Transactions \LaTeX and Microsoft Word Style Files.
	\url{http://www.ieee.org/web/publications/authors/transjnl/index.html}

\end{thebibliography}

% Your document ends here!
\end{document}